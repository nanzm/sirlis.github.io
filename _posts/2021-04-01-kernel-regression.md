---
title: 统计学基础（核回归）
date: 2021-04-01 10:50:49 +0800
categories: [Academic, Knowledge]
tags: [statistics]
math: true
---

本文介绍了统计学中的核回归方法，并铺垫了非参数化统计方法等一些基础知识。

<!--more-->

 ---
 
- [1. 基本知识](#1-基本知识)
  - [1.1. 回归](#11-回归)
  - [1.2. 参数回归](#12-参数回归)
  - [1.3. 非参数回归](#13-非参数回归)
  - [1.4. 近邻回归](#14-近邻回归)
  - [1.5. 核回归](#15-核回归)
- [2. 参考文献](#2-参考文献)

# 1. 基本知识

## 1.1. 回归

回归分析（Regression Analysis）是一种统计学上分析数据的方法，目的在于了解两个或多个变量间是否相关、相关方向与强度，并建立数学模型以便观察特定变量来预测研究者感兴趣的变量。更具体的来说，回归分析可以帮助人们了解在只有一个自变量变化时因变量的变化量。一般来说，通过回归分析我们可以由给出的自变量估计因变量的条件期望。

回归分析是建立因变量 $Y$（或称依变量，反因变量）与自变量 $X$（或称独变量，解释变量）之间关系的模型。简单线性回归使用一个自变量 $X$，复回归使用超过一个自变量（$X_{1},X_{2}...X_{i}$）。

## 1.2. 参数回归

优点：

- 模型形式简单明确，仅由一些参数表达
- 在某些问题中，模型参数具有明确的含义（如经济问题中）
- 当模型参数假设成立时，统计推断的精度较高，能经受实际检验
- 模型能够进行外推运算
- 模型可以用于小样本的统计推断

缺点：

- 回归函数怒的形式需要预先假定
- 模型限制条件较多，一般要求样本满足某种分布，随机误差满足正态假设，解释变量间独立，解释变量与随机误差不相关等
- 模型泛化能力弱、缺乏稳健性

## 1.3. 非参数回归

非参数回归是指并不需要知道总的分布的情况下进行的一种非参数统计方法。

> [非参数统计](https://zh.wikipedia.org/wiki/%E7%84%A1%E6%AF%8D%E6%95%B8%E7%B5%B1%E8%A8%88)（nonparametric statistics），或称非参数统计学，统计学的分支，适用于母群体分布情况未明、小样本、母群体分布不为正态也不易变换为正态。特点在于尽量减少或不修改其建立之模型，较具稳健特性；在样本数不大时，计算过程较简单。
> 
> 非参数统计推断时所使用的统计量的抽样分配通常与总体分配无关，不必推论其中位数、拟合优度、独立性、随机性，更广义的说，非参数统计又称为“不受分布限制统计法”（distribution free）。

优点：

- 回归函数形式自由、受约束少，对数据分布一般不做任何要求
- 适应能力强，稳健性高，回归模型完全由数据驱动
- 对于非线性、非齐次问题效果很好

缺点

- 不能进行外推运算
- 估计的收敛速度慢
- 一般只有在大样本下才能取得很好的效果，小样本效果较差
- 高维诅咒？

## 1.4. 近邻回归

1NN（1-Nearest Neighbor）回归：找寻与输入 $x_q$ 最接近的 $x_i$ 对应的 $y_i$ 作为预测输出。缺点时对大块区域没有数据或数据不足时敏感，拟合的不好。

KNN（K-Nearest Neighbor）回归：找寻 $k$ 个最近邻的点 $x_1,x_2,\cdots,x_k$，然后对他们对应的 $y_1,y_2,\cdots,y_k$ 求**平均**。

加权 kNN （Weighted K-Nearest Neighbor）回归：找寻 $k$ 个最近邻的点 $X_1,X_2,\cdots,X_k$，然后对他们对应的 $y_1,y_2,\cdots,y_k$ 求**加权平均**。权重取法为，离得更近的点具备更大的权重，反之更小。简单的算法为计算距离的倒数，即

$$
\begin{aligned}
y_{q} &= \frac{c_{1}y_{1}+\cdots+c_{k}y_{k}}{\sum_{j=1}^k c_{qj}}\\
c_{qj} &= \frac{1}{distance(x_j,x_q)}
\end{aligned}
$$

影响近邻回归性能的因素为 k 值和距离计算规则。

距离计算一般采用欧式距离、马氏距离、曼哈顿距离等。

对于两个具有 $n$ 维特征的样本点 $\boldsymbol x_i,\boldsymbol x_q$，二者间的欧式距离为

$$
distance(x_i,x_q)=\left( \sum_{k=1}^n \vert x_i^k - x_q^k \vert^2 \right)^{\frac{1}{2}}
$$

## 1.5. 核回归

继续细化权重，提出**核权重**的概念。

$$
c_{qj} = kernel_\lambda(\vert x_j-x_q \vert)
$$

高斯核如下

$$
kernel_\lambda(\vert x_j-x_q \vert)=e^\frac{-(x_i-x_q)^2}{\lambda}
$$

其它核包括均匀分布核、三角核等等，如下图所示。

![](../assets/img/postsimg/20210401/01.jpg)

**核回归就是升级版的加权 KNN，区别在于不是加权 k 个最近的邻居，而是加权所有样本点。**

$$
y_q = \frac{\sum_{i=1}^N c_{qi}y_i}{\sum_{i=1}^Nc_{qi}} = \frac{\sum_{i=1}^N kernel_{\lambda}(distance(x_i,x_q))y_i}{\sum_{i=1}^Nkernel_{\lambda}(distance(x_i,x_q))}
$$

要确定两个东西：

- 核
- $\lambda$

其中，核的选择比 $\lambda$ 的选择更重要。$\lambda$ 的选择根据验证集验证时的验证损失来确定。

![](../assets/img/postsimg/20210401/02.jpg)


# 2. 参考文献

[1] 马同学. [如何直观地理解拉格朗日插值法？](https://www.zhihu.com/question/58333118)

[2] 素_履. [轨迹优化与直接配点法](https://blog.csdn.net/qq_35007540/article/details/105672547)

[3] 百度文库. [龙格库塔法推导](https://wenku.baidu.com/view/98d914413868011ca300a6c30c2259010302f30a.html)