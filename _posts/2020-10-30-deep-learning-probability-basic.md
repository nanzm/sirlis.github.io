---
title: 深度学习基础（概率论）
date: 2020-10-30 10:46:19 +0800
categories: [Knowledge, DeepLearning]
tags: [academic]
math: true
---

本文主要介绍自然语言处理（Natural Language Process，NLP）的基础，然后介绍 Encoder-Decoder（编码-解码）框架和 BERT 框架。

<!--more-->

---
- [1. NLP 介绍](#1-nlp-介绍)
  - [1.1. 文本表示](#11-文本表示)
  - [1.2. 训练思想](#12-训练思想)
- [2. Encoder-Decoder](#2-encoder-decoder)
  - [2.1. RNN E-D](#21-rnn-e-d)
  - [2.2. RNN E-D with attention](#22-rnn-e-d-with-attention)
- [3. Transformer](#3-transformer)
  - [3.1. 简介](#31-简介)
- [4. 参考文献](#4-参考文献)

# 1. 贝叶斯公式

## 概率

**条件概率**：在某条件下事件发生的概率。

**先验概率**：指根据以往经验和分析得到的概率，如全概率公式，它往往作为"由因求果"问题中的"因"出现的概率。

**后验概率**：已知原分布，在实际发生某事件时,是原先某情况的可能性。后验概率是信息理论的基本概念之一。后验概率是指在得到“结果”的信息后重新修正的概率，是“执果寻因”问题中的"果"。先验概率与后验概率有不可分割的联系，后验概率的计算要以先验概率为基础。

事情还没有发生，要求这件事情发生的可能性的大小，是先验概率。事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小，是后验概率。

**后验概率是一种条件概率**。一种解释认为，条件概率是个数学名称，后验概率是建模的时候赋予了一定的意义。一般的条件概率，条件和事件可以是任意的；对于后验概率，它限定了事件为隐变量取值，而条件为观测结果。

## 全概率公式

设 $A_1,...,A_n$ 是样本空间 $S$ 的一个完备事件组，即

- $A_1,...,A_n$ 两两不相容：$A_i \cap A_j = \varnothing\quad (i\neq j)$
- $A_i \cup...\cup A_n = S$

每一次试验中，完备事件组中有且仅有一个事件发生。完备事件组构成样本空间的一个划分。

**全概率公式**。定理：设实验 $E$ 的样本空间为 $S$，$B_1, B_2,...,B_n$ 为 $S$ 的一个划分（完备事件组），且 $P(B_i)>0\quad i=1,2,...n$，$A$ 为 $E$ 的一个事件，则

$$
\begin{aligned}
P(A) &= P(B_1)P(A\vert B_1)+P(B_2)P(A\vert B_2)+...+P(B_n)P(A\vert B_n)\\
&= \sum_{i=1}^n P(B_i)P(A\vert B_i)
\end{aligned}
$$

# 4. 参考文献

无。