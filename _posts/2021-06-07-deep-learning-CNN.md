---
title: 深度学习基础（CNN 卷积神经网络）
date: 2021-06-08 09:08:49 +0800
categories: [Academic, Knowledge]
tags: [deeplearning]
math: true
---

本文介绍了深度学习中卷积神经网络的（CNN）的基础知识。

<!--more-->

 ---
- [1. 基础知识](#1-基础知识)
  - [1.1. 图像](#11-图像)
  - [1.2. 卷积核（kernel）](#12-卷积核kernel)
    - [1.2.1. 单通道卷积](#121-单通道卷积)
    - [1.2.2. 多通道卷积](#122-多通道卷积)
    - [卷积核的参数](#卷积核的参数)
  - [1.3. 滤波器（filter）](#13-滤波器filter)
  - [1.4. 池化](#14-池化)
    - [1.4.1. 最大池化](#141-最大池化)
    - [1.4.2. 平均池化](#142-平均池化)
    - [1.4.3. 池化的好处](#143-池化的好处)
- [2. 参考文献](#2-参考文献)

# 1. 基础知识

## 1.1. 图像

图像在计算机中表示为一个 $height\times width$ 的 2 维的矩阵，矩阵的各个元素取值为颜色的值，可以表示为 $[0,1]$ 区间或 $[0,255]$ 区间的一个数。灰度图像是单通道的，彩色图像是三通道的（R，G，B）。

## 1.2. 卷积核（kernel）

狭义的卷积核（kernel）是一个矩阵，可在图像上滑动并与输入相乘，从而以某种我们期望的方式增强输出。

### 1.2.1. 单通道卷积

假设输入图片为 $6\times 6 \times 1\ channel$，卷积核为 $3\times 3$，stride=1（表示 kernel 的滑动步长为1），padding=1（表示对原始输入图像周围额外增加一层，配合stride=1，可以保证卷积后图片大小不变），则卷积过程如下图所示：

![](./asserts/../../assets/img/postsimg/20210607/00.kernel.gif)

上面的kernel可用于**锐化**图像。比如，考虑下图所示的两个输入图像。

![](./asserts/../../assets/img/postsimg/20210607/00.kerneldemo.jpg)

- 第一个图像，中心值为3 * 5 + 2 * -1 + 2 * -1 + 2 * -1 + 2 * -1 =7，值3增加到7。
- 第二个图像，输出是1 * 5 + 2 * -1 + 2 * -1 + 2 * -1 + 2 * -1 = -3，值1减少到-3。

显然，3和1之间的对比度增加到了7和-3，图像将更清晰锐利。

### 1.2.2. 多通道卷积

假设一个图像为 $5\times 5 \times 3\ channel$，假设卷积核只有1个，卷积核通道为3，每个通道的卷积核大小仍为3x3，padding=0，stride=1。卷积过程如下，每一个通道的像素值与对应的卷积核通道的数值进行卷积，因此每一个通道会对应一个输出卷积结果，三个卷积结果对应位置累加求和，得到最终的卷积结果。



### 卷积核的参数

通过深层 CNN，我们无需再用手工设计的 kernel 来提取特征，而是可以直接学习这些可提取潜在特征的 kernel 值。

卷积核里面数就是所谓的权重，网络训练时输出的结果会和数据集标签做损失计算，然后把计算得到的损失反向梯度下降去更新卷积核里的每一个参数。所以卷积核里面的参数最终是训练得到的。但最开始时是需要给这些参数提供初始值才能使网络运行，可以简略分三种：

- 取偏差很小的高斯分布随机取值
- Xavier 初始化方法
- He kai ming 初始化方法

广义的卷积核与滤波器概念已经发生交融。

卷积后尺寸计算公式：

$$
输出图像尺寸 = (输出图像尺寸-卷积核尺寸 + 2*填充值)/步长+1
$$

## 1.3. 滤波器（filter）

滤波器是卷积核的串联，对应输入图像的每个通道。即比如卷积核是2D的 conv 3×3s1（表示是 $3\times 3$ 卷积核，stride = 1），输入图像 3 通道，那么一个 filter 就是 $3\times 3 \times 3$ 维的。

filter 还包括偏置 bias。

一个卷积层一般包含 $N$ 个 filter，对应输出的 $N$ 张特征图。

## 1.4. 池化

池化层也称下采样层，会压缩输入的特征图，一方面减少了特征，导致了参数减少，进而简化了卷积网络计算时的复杂度；另一方面保持了特征的某种不变性（旋转、平移、伸缩等）。池化的思想来自于视觉机制，是对信息进行抽象的过程。

池化操作主要有两种，一种是平均池化(Average Pooling)，即对邻域内的特征点求平均；另一种是最大池化(Max Pooling)，即对邻域内的特征点取最大。

### 1.4.1. 最大池化

采用 $2\times 2$ 卷积核对 $4\times 4$ 图像的最大池化过程：

![](./asserts/../../assets/img/postsimg/20210607/08.maxpool.jpg)

通常认为如果选取区域均值(mean pooling)，往往能保留整体数据的特征，较好的突出背景信息。

### 1.4.2. 平均池化

采用 $2\times 2$ 卷积核对 $4\times 4$ 图像的平均池化过程：

![](./asserts/../../assets/img/postsimg/20210607/09.avgpool.jpg)

如果选取区域最大值(max pooling)，则能更好保留纹理特征。

### 1.4.3. 池化的好处
> 言有三. [池化是什么意思？](https://www.zhihu.com/question/303215483/answer/615115629)

池化的好处：

- **增大感受野**。所谓感受野，即一个像素对应回原图的区域大小，假如没有pooling，一个3*3，步长为1的卷积，那么输出的一个像素的感受野就是3*3的区域，再加一个stride=1的3*3卷积，则感受野为5*5。假如我们在每一个卷积中间加上3*3的pooling呢？很明显感受野迅速增大，这就是pooling的一大用处。感受野的增加对于模型的能力的提升是必要的，正所谓“一叶障目则不见泰山也”。
- **平移不变性**。我们希望目标的些许位置的移动，能得到相同的结果。因为pooling不断地抽象了区域的特征而不关心位置，所以pooling一定程度上增加了平移不变性。
- **降低优化难度和参数**。我们可以用步长大于1的卷积来替代池化，但是池化每个特征通道单独做降采样，与基于卷积的降采样相比，不需要参数，更容易优化。全局池化更是可以大大降低模型的参数量和优化工作量。


# 2. 参考文献

[1] 维基百科. [Kernel regression](https://en.wikipedia.org/wiki/Kernel_regression)
